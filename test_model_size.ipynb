{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [1, 512, 1, 1]            --\n",
       "├─Conv2d: 1-1                            [1, 64, 32, 32]           1,792\n",
       "├─ReLU: 1-2                              [1, 64, 32, 32]           --\n",
       "├─Conv2d: 1-3                            [1, 64, 32, 32]           36,928\n",
       "├─ReLU: 1-4                              [1, 64, 32, 32]           --\n",
       "├─MaxPool2d: 1-5                         [1, 64, 16, 16]           --\n",
       "├─Conv2d: 1-6                            [1, 128, 16, 16]          73,856\n",
       "├─ReLU: 1-7                              [1, 128, 16, 16]          --\n",
       "├─Conv2d: 1-8                            [1, 128, 16, 16]          147,584\n",
       "├─ReLU: 1-9                              [1, 128, 16, 16]          --\n",
       "├─MaxPool2d: 1-10                        [1, 128, 8, 8]            --\n",
       "├─Conv2d: 1-11                           [1, 256, 8, 8]            295,168\n",
       "├─ReLU: 1-12                             [1, 256, 8, 8]            --\n",
       "├─Conv2d: 1-13                           [1, 256, 8, 8]            590,080\n",
       "├─ReLU: 1-14                             [1, 256, 8, 8]            --\n",
       "├─Conv2d: 1-15                           [1, 256, 8, 8]            590,080\n",
       "├─ReLU: 1-16                             [1, 256, 8, 8]            --\n",
       "├─MaxPool2d: 1-17                        [1, 256, 4, 4]            --\n",
       "├─Conv2d: 1-18                           [1, 512, 4, 4]            1,180,160\n",
       "├─ReLU: 1-19                             [1, 512, 4, 4]            --\n",
       "├─Conv2d: 1-20                           [1, 512, 4, 4]            2,359,808\n",
       "├─ReLU: 1-21                             [1, 512, 4, 4]            --\n",
       "├─Conv2d: 1-22                           [1, 512, 4, 4]            2,359,808\n",
       "├─ReLU: 1-23                             [1, 512, 4, 4]            --\n",
       "├─MaxPool2d: 1-24                        [1, 512, 2, 2]            --\n",
       "├─Conv2d: 1-25                           [1, 512, 2, 2]            2,359,808\n",
       "├─ReLU: 1-26                             [1, 512, 2, 2]            --\n",
       "├─Conv2d: 1-27                           [1, 512, 2, 2]            2,359,808\n",
       "├─ReLU: 1-28                             [1, 512, 2, 2]            --\n",
       "├─Conv2d: 1-29                           [1, 512, 2, 2]            2,359,808\n",
       "├─ReLU: 1-30                             [1, 512, 2, 2]            --\n",
       "├─MaxPool2d: 1-31                        [1, 512, 1, 1]            --\n",
       "==========================================================================================\n",
       "Total params: 14,714,688\n",
       "Trainable params: 14,714,688\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 313.47\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 2.21\n",
       "Params size (MB): 58.86\n",
       "Estimated Total Size (MB): 61.08\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "vgg = models.vgg16(pretrained=True).features\n",
    "summary(vgg.to(device), input_size=(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "AE                                       [1, 3, 32, 32]            --\n",
       "├─AELREncoder: 1-1                       [1, 64]                   --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 16, 16]           128\n",
       "│    └─Conv2d: 2-2                       [1, 64, 8, 8]             32,832\n",
       "│    └─Conv2d: 2-3                       [1, 128, 4, 4]            131,200\n",
       "│    └─Linear: 2-4                       [1, 64]                   131,136\n",
       "├─AEDecoder: 1-2                         [1, 3, 32, 32]            --\n",
       "│    └─Linear: 2-5                       [1, 4096]                 266,240\n",
       "│    └─ConvTranspose2d: 2-6              [1, 128, 8, 8]            524,416\n",
       "│    └─ConvTranspose2d: 2-7              [1, 64, 16, 16]           131,136\n",
       "│    └─ConvTranspose2d: 2-8              [1, 32, 32, 32]           32,800\n",
       "│    └─Conv2d: 2-9                       [1, 3, 32, 32]            99\n",
       "==========================================================================================\n",
       "Total params: 1,249,987\n",
       "Trainable params: 1,249,987\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 105.45\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.63\n",
       "Params size (MB): 5.00\n",
       "Estimated Total Size (MB): 5.63\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.prediction.AE import AE\n",
    "\n",
    "latent_dims = [64]\n",
    "channels = [32, 64, 128, 256]\n",
    "in_channels = 3\n",
    "base_width = 16\n",
    "model = AE(in_channels=in_channels,\n",
    "                          latent_dims=latent_dims,\n",
    "                          channels=channels,\n",
    "                          base_width=base_width\n",
    "                          ).to(device)\n",
    "print(model.sample(torch.randn(1, in_channels, base_width, base_width).to(device)).shape)\n",
    "summary(model, input_size=(1, in_channels, base_width, base_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VAE                                      [1, 3, 32, 32]            --\n",
       "├─VAELREncoder: 1-1                      [1, 64]                   --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 16, 16]           128\n",
       "│    └─Conv2d: 2-2                       [1, 64, 8, 8]             32,832\n",
       "│    └─Conv2d: 2-3                       [1, 128, 4, 4]            131,200\n",
       "│    └─Linear: 2-4                       [1, 64]                   131,136\n",
       "│    └─Linear: 2-5                       [1, 64]                   131,136\n",
       "├─VAEDecoder: 1-2                        [1, 3, 32, 32]            --\n",
       "│    └─Linear: 2-6                       [1, 4096]                 266,240\n",
       "│    └─ConvTranspose2d: 2-7              [1, 128, 8, 8]            524,416\n",
       "│    └─ConvTranspose2d: 2-8              [1, 64, 16, 16]           131,136\n",
       "│    └─ConvTranspose2d: 2-9              [1, 32, 32, 32]           32,800\n",
       "│    └─Conv2d: 2-10                      [1, 3, 32, 32]            99\n",
       "==========================================================================================\n",
       "Total params: 1,381,123\n",
       "Trainable params: 1,381,123\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 105.58\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.63\n",
       "Params size (MB): 5.52\n",
       "Estimated Total Size (MB): 6.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.prediction.VAE import VAE\n",
    "\n",
    "latent_dims = [64]\n",
    "channels = [32, 64, 128, 256]\n",
    "in_channels = 3\n",
    "base_width = 16\n",
    "model = VAE(in_channels=in_channels,\n",
    "                          latent_dims=latent_dims,\n",
    "                          channels=channels,\n",
    "                          base_width=base_width\n",
    "                          ).to(device)\n",
    "print(model.sample(torch.randn(1, in_channels, base_width, base_width).to(device)).shape)\n",
    "summary(model, input_size=(1, in_channels, base_width, base_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "HVAE                                     [1, 3, 32, 32]            --\n",
       "├─HVAELREncoder: 1-1                     [1, 16]                   --\n",
       "│    └─Conv2d: 2-1                       [1, 16, 16, 16]           64\n",
       "│    └─Conv2d: 2-2                       [1, 32, 8, 8]             8,224\n",
       "│    └─Conv2d: 2-3                       [1, 64, 4, 4]             32,832\n",
       "│    └─Linear: 2-4                       [1, 16]                   16,400\n",
       "│    └─Linear: 2-5                       [1, 16]                   16,400\n",
       "│    └─Linear: 2-6                       [1, 32]                   65,568\n",
       "│    └─Linear: 2-7                       [1, 32]                   65,568\n",
       "│    └─Linear: 2-8                       [1, 48]                   196,656\n",
       "│    └─Linear: 2-9                       [1, 48]                   196,656\n",
       "├─HVAEDecoder: 1-2                       [1, 3, 32, 32]            --\n",
       "│    └─Linear: 2-10                      [1, 2048]                 34,816\n",
       "│    └─Linear: 2-11                      [1, 4096]                 135,168\n",
       "│    └─Linear: 2-12                      [1, 8192]                 401,408\n",
       "│    └─ConvTranspose2d: 2-13             [1, 64, 8, 8]             131,136\n",
       "│    └─ConvTranspose2d: 2-14             [1, 32, 16, 16]           32,800\n",
       "│    └─ConvTranspose2d: 2-15             [1, 16, 32, 32]           8,208\n",
       "│    └─Conv2d: 2-16                      [1, 3, 32, 32]            51\n",
       "==========================================================================================\n",
       "Total params: 1,341,955\n",
       "Trainable params: 1,341,955\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 27.44\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.43\n",
       "Params size (MB): 5.37\n",
       "Estimated Total Size (MB): 5.80\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.prediction.HVAE import HVAE\n",
    "\n",
    "latent_dims = [16, 32, 48]\n",
    "channels = [16, 32, 64, 128]\n",
    "in_channels = 3\n",
    "base_width = 16\n",
    "model = HVAE(in_channels=in_channels,\n",
    "                          latent_dims=latent_dims,\n",
    "                          channels=channels,\n",
    "                          base_width=base_width\n",
    "                          ).to(device)\n",
    "print(model.sample(torch.randn(1, in_channels, base_width, base_width).to(device)).shape)\n",
    "summary(model, input_size=(1, in_channels, base_width, base_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "ConditionalHierarchicalVAE                    [1, 3, 32, 32]            --\n",
       "├─ConditionalHierarchicalEncoder: 1-1         [1, 16]                   --\n",
       "│    └─Conv2d: 2-1                            [1, 16, 32, 32]           64\n",
       "│    └─Conv2d: 2-2                            [1, 32, 16, 16]           8,224\n",
       "│    └─Conv2d: 2-3                            [1, 64, 8, 8]             32,832\n",
       "│    └─Conv2d: 2-4                            [1, 96, 4, 4]             98,400\n",
       "│    └─Linear: 2-5                            [1, 16]                   24,592\n",
       "│    └─Linear: 2-6                            [1, 16]                   24,592\n",
       "│    └─Linear: 2-7                            [1, 32]                   131,104\n",
       "│    └─Linear: 2-8                            [1, 32]                   131,104\n",
       "│    └─Linear: 2-9                            [1, 48]                   393,264\n",
       "│    └─Linear: 2-10                           [1, 48]                   393,264\n",
       "├─ConditionalHierarchicalLREncoder: 1-2       [1, 32]                   --\n",
       "│    └─Conv2d: 2-11                           [1, 16, 16, 16]           64\n",
       "│    └─Conv2d: 2-12                           [1, 32, 8, 8]             8,224\n",
       "│    └─Linear: 2-13                           [1, 32]                   65,568\n",
       "│    └─Linear: 2-14                           [1, 48]                   196,656\n",
       "├─ConditionalHierarchicalDecoder: 1-3         [1, 3, 32, 32]            --\n",
       "│    └─Linear: 2-15                           [1, 1536]                 26,112\n",
       "│    └─Linear: 2-16                           [1, 4096]                 135,168\n",
       "│    └─Linear: 2-17                           [1, 8192]                 401,408\n",
       "│    └─Linear: 2-18                           [1, 1536]                 50,688\n",
       "│    └─Linear: 2-19                           [1, 4096]                 200,704\n",
       "│    └─ConvTranspose2d: 2-20                  [1, 64, 8, 8]             196,672\n",
       "│    └─ConvTranspose2d: 2-21                  [1, 32, 16, 16]           65,568\n",
       "│    └─ConvTranspose2d: 2-22                  [1, 16, 32, 32]           8,208\n",
       "│    └─Conv2d: 2-23                           [1, 3, 32, 32]            51\n",
       "===============================================================================================\n",
       "Total params: 2,592,531\n",
       "Trainable params: 2,592,531\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 46.39\n",
       "===============================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.70\n",
       "Params size (MB): 10.37\n",
       "Estimated Total Size (MB): 11.09\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.prediction.HVAE import ConditionalHierarchicalVAE\n",
    "\n",
    "latent_dims = [16, 32, 48]\n",
    "condition_dims = [32, 48]\n",
    "channels = [16, 32, 64, 96]\n",
    "cond_channels = [16, 32]\n",
    "in_channels = 3\n",
    "cond_base_width = 16\n",
    "model = ConditionalHierarchicalVAE(in_channels=in_channels,\n",
    "                          latent_dims=latent_dims,\n",
    "                          channels=channels,\n",
    "                          cond_channels=cond_channels,\n",
    "                          condition_dims=condition_dims,\n",
    "                          cond_base_width=cond_base_width\n",
    "                          ).to(device)\n",
    "print(model.sample(torch.randn(1, in_channels, cond_base_width, cond_base_width).to(device)).shape)\n",
    "summary(model, input_size=[(1, in_channels, 32, 32), (1, in_channels, cond_base_width, cond_base_width)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ConditionalVAE                           [1, 3, 32, 32]            --\n",
       "├─ConditionalVAEEncoder: 1-1             [1, 64]                   --\n",
       "│    └─Conv2d: 2-1                       [1, 16, 32, 32]           64\n",
       "│    └─Conv2d: 2-2                       [1, 32, 16, 16]           8,224\n",
       "│    └─Conv2d: 2-3                       [1, 64, 8, 8]             32,832\n",
       "│    └─Conv2d: 2-4                       [1, 128, 4, 4]            131,200\n",
       "│    └─Linear: 2-5                       [1, 64]                   131,136\n",
       "│    └─Linear: 2-6                       [1, 64]                   131,136\n",
       "├─ConditionalVAELREncoder: 1-2           [1, 32]                   --\n",
       "│    └─Conv2d: 2-7                       [1, 16, 16, 16]           64\n",
       "│    └─Conv2d: 2-8                       [1, 32, 8, 8]             8,224\n",
       "│    └─Linear: 2-9                       [1, 32]                   65,568\n",
       "│    └─Linear: 2-10                      [1, 64]                   262,208\n",
       "├─ConditionalVAEDecoder: 1-3             [1, 3, 32, 32]            --\n",
       "│    └─Linear: 2-11                      [1, 2048]                 133,120\n",
       "│    └─Linear: 2-12                      [1, 2048]                 67,584\n",
       "│    └─Linear: 2-13                      [1, 4096]                 266,240\n",
       "│    └─ConvTranspose2d: 2-14             [1, 64, 8, 8]             262,208\n",
       "│    └─ConvTranspose2d: 2-15             [1, 32, 16, 16]           65,568\n",
       "│    └─ConvTranspose2d: 2-16             [1, 16, 32, 32]           8,208\n",
       "│    └─Conv2d: 2-17                      [1, 3, 32, 32]            51\n",
       "==========================================================================================\n",
       "Total params: 1,573,635\n",
       "Trainable params: 1,573,635\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 49.99\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.62\n",
       "Params size (MB): 6.29\n",
       "Estimated Total Size (MB): 6.93\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.prediction.VAE import ConditionalVAE\n",
    "\n",
    "latent_dims = [64]\n",
    "condition_dims = [32, 64]\n",
    "channels = [16, 32, 64, 128]\n",
    "cond_channels = [16, 32]\n",
    "in_channels = 3\n",
    "cond_base_width = 16\n",
    "model = ConditionalVAE(in_channels=in_channels,\n",
    "                          latent_dims=latent_dims,\n",
    "                          channels=channels,\n",
    "                          cond_channels=cond_channels,\n",
    "                          condition_dims=condition_dims,\n",
    "                          cond_base_width=cond_base_width\n",
    "                          ).to(device)\n",
    "print(model.sample(torch.randn(1, in_channels, cond_base_width, cond_base_width).to(device)).shape)\n",
    "summary(model, input_size=[(1, in_channels, 32, 32), (1, in_channels, cond_base_width, cond_base_width)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "AE                                       [1, 3, 32, 32]            --\n",
       "├─AELREncoder: 1-1                       [1, 64]                   --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 16, 16]           128\n",
       "│    └─Conv2d: 2-2                       [1, 64, 8, 8]             32,832\n",
       "│    └─Conv2d: 2-3                       [1, 128, 4, 4]            131,200\n",
       "│    └─Linear: 2-4                       [1, 64]                   131,136\n",
       "├─AEDecoder: 1-2                         [1, 3, 32, 32]            --\n",
       "│    └─Linear: 2-5                       [1, 4096]                 266,240\n",
       "│    └─ConvTranspose2d: 2-6              [1, 128, 8, 8]            524,416\n",
       "│    └─ConvTranspose2d: 2-7              [1, 64, 16, 16]           131,136\n",
       "│    └─ConvTranspose2d: 2-8              [1, 32, 32, 32]           32,800\n",
       "│    └─Conv2d: 2-9                       [1, 3, 32, 32]            99\n",
       "├─Conv2d: 1-3                            [1, 3, 32, 32]            21\n",
       "==========================================================================================\n",
       "Total params: 1,250,008\n",
       "Trainable params: 1,250,008\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 105.47\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.66\n",
       "Params size (MB): 5.00\n",
       "Estimated Total Size (MB): 5.67\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.completion.AE import AE\n",
    "\n",
    "latent_dims = [64]\n",
    "channels = [32, 64, 128, 256]\n",
    "in_channels = 3\n",
    "base_width = 16\n",
    "model = AE(in_channels=in_channels,\n",
    "                          latent_dims=latent_dims,\n",
    "                          channels=channels,\n",
    "                          base_width=base_width\n",
    "                          ).to(device)\n",
    "print(model.sample(torch.randn(1, in_channels, base_width, base_width).to(device)).shape)\n",
    "summary(model, input_size=[(1, in_channels, base_width, base_width), (1, in_channels, 32, 32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VAE                                      [1, 3, 32, 32]            --\n",
       "├─VAELREncoder: 1-1                      [1, 64]                   --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 16, 16]           128\n",
       "│    └─Conv2d: 2-2                       [1, 64, 8, 8]             32,832\n",
       "│    └─Conv2d: 2-3                       [1, 128, 4, 4]            131,200\n",
       "│    └─Linear: 2-4                       [1, 64]                   131,136\n",
       "│    └─Linear: 2-5                       [1, 64]                   131,136\n",
       "├─VAEDecoder: 1-2                        [1, 3, 32, 32]            --\n",
       "│    └─Linear: 2-6                       [1, 4096]                 266,240\n",
       "│    └─ConvTranspose2d: 2-7              [1, 128, 8, 8]            524,416\n",
       "│    └─ConvTranspose2d: 2-8              [1, 64, 16, 16]           131,136\n",
       "│    └─ConvTranspose2d: 2-9              [1, 32, 32, 32]           32,800\n",
       "│    └─Conv2d: 2-10                      [1, 3, 32, 32]            99\n",
       "├─Conv2d: 1-3                            [1, 3, 32, 32]            21\n",
       "==========================================================================================\n",
       "Total params: 1,381,144\n",
       "Trainable params: 1,381,144\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 105.61\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.66\n",
       "Params size (MB): 5.52\n",
       "Estimated Total Size (MB): 6.20\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.completion.VAE import VAE\n",
    "\n",
    "latent_dims = [64]\n",
    "channels = [32, 64, 128, 256]\n",
    "in_channels = 3\n",
    "base_width = 16\n",
    "model = VAE(in_channels=in_channels,\n",
    "                          latent_dims=latent_dims,\n",
    "                          channels=channels,\n",
    "                          base_width=base_width\n",
    "                          ).to(device)\n",
    "print(model.sample(torch.randn(1, in_channels, base_width, base_width).to(device)).shape)\n",
    "summary(model, input_size=[(1, in_channels, base_width, base_width), (1, in_channels, 32, 32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "HVAE                                     [1, 3, 32, 32]            --\n",
       "├─HVAELREncoder: 1-1                     [1, 64]                   --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 16, 16]           128\n",
       "│    └─Conv2d: 2-2                       [1, 64, 8, 8]             32,832\n",
       "│    └─Conv2d: 2-3                       [1, 128, 4, 4]            131,200\n",
       "│    └─Linear: 2-4                       [1, 64]                   131,136\n",
       "│    └─Linear: 2-5                       [1, 64]                   131,136\n",
       "│    └─Linear: 2-6                       [1, 128]                  524,416\n",
       "│    └─Linear: 2-7                       [1, 128]                  524,416\n",
       "│    └─Linear: 2-8                       [1, 256]                  2,097,408\n",
       "│    └─Linear: 2-9                       [1, 256]                  2,097,408\n",
       "├─HVAEDecoder: 1-2                       [1, 3, 32, 32]            --\n",
       "│    └─Linear: 2-10                      [1, 4096]                 266,240\n",
       "│    └─Linear: 2-11                      [1, 8192]                 1,056,768\n",
       "│    └─Linear: 2-12                      [1, 16384]                4,210,688\n",
       "│    └─ConvTranspose2d: 2-13             [1, 128, 8, 8]            524,416\n",
       "│    └─ConvTranspose2d: 2-14             [1, 64, 16, 16]           131,136\n",
       "│    └─ConvTranspose2d: 2-15             [1, 32, 32, 32]           32,800\n",
       "│    └─Conv2d: 2-16                      [1, 3, 32, 32]            99\n",
       "├─Conv2d: 1-3                            [1, 3, 32, 32]            21\n",
       "==========================================================================================\n",
       "Total params: 11,892,248\n",
       "Trainable params: 11,892,248\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 116.12\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.86\n",
       "Params size (MB): 47.57\n",
       "Estimated Total Size (MB): 48.44\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.completion.HVAE import HVAE\n",
    "\n",
    "latent_dims = [64, 128, 256]\n",
    "channels = [32, 64, 128, 256]\n",
    "in_channels = 3\n",
    "base_width = 16\n",
    "model = HVAE(in_channels=in_channels,\n",
    "                          latent_dims=latent_dims,\n",
    "                          channels=channels,\n",
    "                          base_width=base_width\n",
    "                          ).to(device)\n",
    "print(model.sample(torch.randn(1, in_channels, base_width, base_width).to(device)).shape)\n",
    "summary(model, input_size=[(1, in_channels, base_width, base_width), (1, in_channels, 32, 32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "ConditionalHierarchicalVAE                    [1, 3, 32, 32]            --\n",
       "├─ConditionalHierarchicalEncoder: 1-1         [1, 16]                   --\n",
       "│    └─Conv2d: 2-1                            [1, 16, 32, 32]           64\n",
       "│    └─Conv2d: 2-2                            [1, 32, 16, 16]           8,224\n",
       "│    └─Conv2d: 2-3                            [1, 64, 8, 8]             32,832\n",
       "│    └─Conv2d: 2-4                            [1, 128, 4, 4]            131,200\n",
       "│    └─Linear: 2-5                            [1, 16]                   32,784\n",
       "│    └─Linear: 2-6                            [1, 16]                   32,784\n",
       "│    └─Linear: 2-7                            [1, 32]                   131,104\n",
       "│    └─Linear: 2-8                            [1, 32]                   131,104\n",
       "│    └─Linear: 2-9                            [1, 64]                   524,352\n",
       "│    └─Linear: 2-10                           [1, 64]                   524,352\n",
       "├─ConditionalHierarchicalLREncoder: 1-2       [1, 32]                   --\n",
       "│    └─Conv2d: 2-11                           [1, 16, 16, 16]           64\n",
       "│    └─Conv2d: 2-12                           [1, 32, 8, 8]             8,224\n",
       "│    └─Linear: 2-13                           [1, 32]                   65,568\n",
       "│    └─Linear: 2-14                           [1, 64]                   262,208\n",
       "├─ConditionalHierarchicalDecoder: 1-3         [1, 3, 32, 32]            --\n",
       "│    └─Linear: 2-15                           [1, 2048]                 34,816\n",
       "│    └─Linear: 2-16                           [1, 4096]                 135,168\n",
       "│    └─Linear: 2-17                           [1, 8192]                 532,480\n",
       "│    └─Linear: 2-18                           [1, 2048]                 67,584\n",
       "│    └─Linear: 2-19                           [1, 4096]                 266,240\n",
       "│    └─ConvTranspose2d: 2-20                  [1, 64, 8, 8]             262,208\n",
       "│    └─ConvTranspose2d: 2-21                  [1, 32, 16, 16]           65,568\n",
       "│    └─ConvTranspose2d: 2-22                  [1, 16, 32, 32]           8,208\n",
       "│    └─Conv2d: 2-23                           [1, 3, 32, 32]            51\n",
       "├─Conv2d: 1-4                                 [1, 3, 32, 32]            21\n",
       "===============================================================================================\n",
       "Total params: 3,257,208\n",
       "Trainable params: 3,257,208\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 51.70\n",
       "===============================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 0.74\n",
       "Params size (MB): 13.03\n",
       "Estimated Total Size (MB): 13.80\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.completion.HVAE import ConditionalHierarchicalVAE\n",
    "\n",
    "latent_dims = [16, 32, 64]\n",
    "condition_dims = [32, 64]\n",
    "channels = [16, 32, 64, 128]\n",
    "cond_channels = [16, 32]\n",
    "in_channels = 3\n",
    "cond_base_width = 16\n",
    "model = ConditionalHierarchicalVAE(in_channels=in_channels,\n",
    "                          latent_dims=latent_dims,\n",
    "                          channels=channels,\n",
    "                          cond_channels=cond_channels,\n",
    "                          condition_dims=condition_dims,\n",
    "                          cond_base_width=cond_base_width\n",
    "                          ).to(device)\n",
    "print(model.sample(torch.randn(1, in_channels, cond_base_width, cond_base_width).to(device)).shape)\n",
    "summary(model, input_size=[(1, in_channels, 32, 32), (1, in_channels, cond_base_width, cond_base_width), (1, in_channels, 32, 32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ConditionalVAE                           [1, 3, 32, 32]            --\n",
       "├─ConditionalVAEEncoder: 1-1             [1, 64]                   --\n",
       "│    └─Conv2d: 2-1                       [1, 16, 32, 32]           64\n",
       "│    └─Conv2d: 2-2                       [1, 32, 16, 16]           8,224\n",
       "│    └─Conv2d: 2-3                       [1, 64, 8, 8]             32,832\n",
       "│    └─Conv2d: 2-4                       [1, 128, 4, 4]            131,200\n",
       "│    └─Linear: 2-5                       [1, 64]                   131,136\n",
       "│    └─Linear: 2-6                       [1, 64]                   131,136\n",
       "├─ConditionalVAELREncoder: 1-2           [1, 32]                   --\n",
       "│    └─Conv2d: 2-7                       [1, 16, 16, 16]           64\n",
       "│    └─Conv2d: 2-8                       [1, 32, 8, 8]             8,224\n",
       "│    └─Linear: 2-9                       [1, 32]                   65,568\n",
       "│    └─Linear: 2-10                      [1, 64]                   262,208\n",
       "├─ConditionalVAEDecoder: 1-3             [1, 3, 32, 32]            --\n",
       "│    └─Linear: 2-11                      [1, 2048]                 133,120\n",
       "│    └─Linear: 2-12                      [1, 2048]                 67,584\n",
       "│    └─Linear: 2-13                      [1, 4096]                 266,240\n",
       "│    └─ConvTranspose2d: 2-14             [1, 64, 8, 8]             262,208\n",
       "│    └─ConvTranspose2d: 2-15             [1, 32, 16, 16]           65,568\n",
       "│    └─ConvTranspose2d: 2-16             [1, 16, 32, 32]           8,208\n",
       "│    └─Conv2d: 2-17                      [1, 3, 32, 32]            51\n",
       "├─Conv2d: 1-4                            [1, 3, 32, 32]            21\n",
       "==========================================================================================\n",
       "Total params: 1,573,656\n",
       "Trainable params: 1,573,656\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 50.02\n",
       "==========================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 0.64\n",
       "Params size (MB): 6.29\n",
       "Estimated Total Size (MB): 6.96\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.completion.VAE import ConditionalVAE\n",
    "\n",
    "latent_dims = [64]\n",
    "condition_dims = [32, 64]\n",
    "channels = [16, 32, 64, 128]\n",
    "cond_channels = [16, 32]\n",
    "in_channels = 3\n",
    "cond_base_width = 16\n",
    "model = ConditionalVAE(in_channels=in_channels,\n",
    "                          latent_dims=latent_dims,\n",
    "                          channels=channels,\n",
    "                          cond_channels=cond_channels,\n",
    "                          condition_dims=condition_dims,\n",
    "                          cond_base_width=cond_base_width\n",
    "                          ).to(device)\n",
    "print(model.sample(torch.randn(1, in_channels, cond_base_width, cond_base_width).to(device)).shape)\n",
    "summary(model, input_size=[(1, in_channels, 32, 32), (1, in_channels, cond_base_width, cond_base_width), (1, in_channels, 32, 32)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/miniconda3/envs/ML/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/max/miniconda3/envs/ML/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 10.74770450592041\n",
      "SSIM: 0.01117071881890297\n",
      "LPIPS: 0.8701097369194031\n",
      "BRISQUE: 52.5799560546875\n",
      "PSNR Consistency: 10.29944133758545\n",
      "MSE Diversity: 4.640843506820147e-06\n",
      "LPIPS Diversity: 0.012734387777527445\n"
     ]
    }
   ],
   "source": [
    "from models.prediction.VAE import VAE\n",
    "from utils.metrics import validation_scores\n",
    "\n",
    "latent_dims = [64]\n",
    "channels = [32, 64, 128, 256]\n",
    "in_channels = 3\n",
    "base_width = 16\n",
    "model = VAE(in_channels=in_channels,\n",
    "                          latent_dims=latent_dims,\n",
    "                          channels=channels,\n",
    "                          base_width=base_width\n",
    "                          ).to(device)\n",
    "\n",
    "HR = torch.rand(32, in_channels, 32, 32).to(device)\n",
    "LR = torch.rand(32, in_channels, base_width, base_width).to(device)\n",
    "psnr_val, ssim_val, lpips_val, brisque_val, psnr_consistency_val, mse_diversity_val, lpips_diversity_val = validation_scores(model, HR, LR)\n",
    "print(\"PSNR:\", psnr_val)\n",
    "print(\"SSIM:\", ssim_val)\n",
    "print(\"LPIPS:\", lpips_val)\n",
    "print(\"BRISQUE:\", brisque_val)\n",
    "print(\"PSNR Consistency:\", psnr_consistency_val)\n",
    "print(\"MSE Diversity:\", mse_diversity_val)\n",
    "print(\"LPIPS Diversity:\", lpips_diversity_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
